<h1 id="problem-statement">Problem Statement</h1>
<p>The goal of this project is to replicate the results obtained in the
paper, as well as perform a few ablation studies on them. While we are
able to replicate the results as given in the paper, the ablations
ask:</p>
<ol type="1">
<li>Can we express the RP iteration as a continuous-time ODE, and solve
it using ODE solution methods? In particular we use RK4 with timestep of
0.2 to examine if it has any performance improvements.</li>
<li>Could we improve the performance of the model by incorporating a
complement channel (<span
class="math inline"><em>A</em><sup><em>c</em></sup></span>) containing
global information?</li>
</ol>
<h1 id="approach-and-results">Approach and Results</h1>
<h2 id="residual-propagation">Residual Propagation</h2>
<p>The baseline RP equation is: <span class="math display">$$ \[
R_{t+1}, R^{`}_{t+1} \] = -\eta A^{k} \[ R_t, 0 \] + \[R_t, R^{`}_{t} \]
$$</span></p>
<p>It is implemented in the following class in <code>main.py</code>:</p>
<pre><code>    ```class LabelPropagation(nn.Module):
    &#39;&#39;&#39;
    based on paper &#39;Learning with Local and Global Consistency&#39;
    input:
        redisuals: [instances, k], val and test instances are masked by 0
    output:
        y_pred: [instances, k]
    &#39;&#39;&#39;
    def __init__(self, k, alpha):
        super().__init__()
        self.k = k
        self.alpha = alpha
        self.A = None

    def preprocess(self, args, edge_index, num_nodes, num_classes):
        self.A = construct_sparse_adj(edge_index, num_nodes=num_nodes, type=&#39;DAD&#39;)

    def forward(self, redisuals):
        if self.alpha &lt; 1: 
            y_pred = redisuals.clone()
            for _ in range(self.k):
                y_pred = self.alpha * self.A @ y_pred + (1 - self.alpha) * redisuals
            return y_pred
        else:
            for _ in range(self.k):
                redisuals = self.A @ redisuals
            return redisuals```</code></pre>
<p>Here <span class="math inline"><em>A</em></span> is the adjacency
matrix of the graph we are interested in and <span
class="math inline"><em>R</em><sub><em>t</em></sub></span> is the
residual of the training data (nodes) at time <span
class="math inline"><em>t</em></span>. Test nodes have estimated
residuals stored in <span
class="math inline"><em>R</em><sub><em>t</em></sub><sup>′</sup></span>.</p>
<p>The ablation was run with a modified <code>run.sh</code> file. As
suggested in the paper, we ran a hyperparameter search over learning
rates (<code>lr</code>) from 0.01 to 1 in seven steps, and for each
learning rate, we tried ten values of <span
class="math inline"><em>k</em></span> - from 1 to 10. Performance was
exhaustively logged in <code>run_log.txt</code>, but only for steps
where it improved more than 2 decimal places, since it was observed that
for many steps the performance did not change much. In order to see how
much better Generalized RP was than RP in heterophilic settings, we also
ran it on datasets like <code>amazon-ratings</code> and
<code>roman-empire</code>. Our performance summaries are as below, with
the quantities in brackets representing the values quoted in the paper.
A * means that the quantities are quoted when running generalized RP on
the dataset in the paper, but we have used vanilla RP for homophilic
datasets here.</p>
<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 27%" />
<col style="width: 28%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Best Validation Accuracy</th>
<th>Best Test Accuracy</th>
<th>Best Hyperparameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ogbn-arxiv</td>
<td>73.18 (71.37)</td>
<td>70.08 (70.06)</td>
<td>lr = 0.05, k = 7</td>
</tr>
<tr class="even">
<td>cora</td>
<td>70.60</td>
<td>71.40 (82.7*)</td>
<td>lr = 0.02, k = 10</td>
</tr>
<tr class="odd">
<td>citeseer</td>
<td>48.40</td>
<td>48.00 (73.0*)</td>
<td>lr = 0.01, k = 9</td>
</tr>
<tr class="even">
<td>pubmed</td>
<td>72.80</td>
<td>71.90 (80.1.0*)</td>
<td>lr = 0.02, k = 8</td>
</tr>
<tr class="odd">
<td>roman-empire</td>
<td>8.44</td>
<td>8.58 (66.01 ± 0.56*)</td>
<td>lr = 1.0, k = 3</td>
</tr>
<tr class="even">
<td>amazon-ratings</td>
<td>49.19</td>
<td>48.64 (47.95 ± 0.57)</td>
<td>lr = 0.05, k = 4</td>
</tr>
</tbody>
</table>
<p>We see that vanilla RP can be competitive in some scenarios to
Generalized RP, especially in homophilic graphs, though there are some
situations where it is utterly abysmal (see roman-empire).</p>
<h3 id="ablation-a-continuous-time-rk4">Ablation A: Continuous-time
RK4</h3>
<p>We write the ODE as</p>
<pre><code>    ```class RK4Propagation(nn.Module):
        def __init__(self, k, eta, dt=0.2):
            super().__init__()
            self.k = k
            self.eta = eta
            self.dt = dt
            self.A = None
            self.train_mask = None

        def preprocess(self, args, edge_index, num_nodes, train_mask):
            self.A = construct_sparse_adj(edge_index, num_nodes=num_nodes, type=&#39;DAD&#39;).to(args.device)
            self.train_mask = train_mask.to(args.device)

        def apply_Ak_B(self, r):
            # Apply B (mask out non-train nodes)
            r = r * self.train_mask.unsqueeze(-1)
            # Apply A k times
            for _ in range(self.k):
                r = torch.sparse.mm(self.A, r)
            return -self.eta * r

        def propagate(self, r0):
            steps = int(1.0 / self.dt)
            out = [r0]
            r = r0
            for _ in range(steps):
                k1 = self.apply_Ak_B(r)
                k2 = self.apply_Ak_B(r + 0.5 * self.dt * k1)
                k3 = self.apply_Ak_B(r + 0.5 * self.dt * k2)
                k4 = self.apply_Ak_B(r + self.dt * k3)
                r = r + (self.dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)
                out.append(r)
            return out```</code></pre>
<p>Essentially replacing the discrete time update by <span
class="math inline">$\frac{dx}{dt} = -eta BA^k x$</span> where <span
class="math inline"><em>x</em></span> is the vector of residuals, <span
class="math inline"><em>B</em></span> is the filter matrix that is a
diagonal matrix with 1s for the training nodes and 0s everywhere else.
In principle this is solvable exactly, but to do so ended up requiring
massive amounts of memory (100 GB+ of RAM to store the dense matrices
generated by <span class="math inline">$\exp(-\etaBAt)$</span>). As a
result we ended up using RK4. Nonethless, the idea was to split each
discrete timestep into five timesteps of 0.2 units each, to see if
increaing the resolution of the model would help its performance. In a
nutshell, it did not help or harm performance, though it took twice as
long to run.</p>
<p>Here is a summary of its performance on different datasets. Here
baseline refers to the results above.</p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 14%" />
<col style="width: 12%" />
<col style="width: 16%" />
<col style="width: 13%" />
<col style="width: 17%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Best Val Acc (Baseline)</th>
<th>Best Val Acc (RK4)</th>
<th>Best Test Acc (Baseline)</th>
<th>Best Test Acc (RK4)</th>
<th>Best Hyperparams (Baseline)</th>
<th>Best Hyperparams (RK4)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ogbn-arxiv</td>
<td>73.18</td>
<td>71.37</td>
<td>70.08</td>
<td>70.07</td>
<td>lr = 0.05, k = 7</td>
<td>lr = 0.5, k = 7</td>
</tr>
<tr class="even">
<td>cora</td>
<td>70.60</td>
<td>70.60</td>
<td>71.40</td>
<td>71.40</td>
<td>lr = 0.02, k = 10</td>
<td>lr = 0.2, k = 10</td>
</tr>
<tr class="odd">
<td>citeseer</td>
<td>48.40</td>
<td>48.40</td>
<td>48.00</td>
<td>48.00</td>
<td>lr = 0.01, k = 9</td>
<td>lr = 0.05, k = 9</td>
</tr>
<tr class="even">
<td>pubmed</td>
<td>72.80</td>
<td>72.80</td>
<td>71.90</td>
<td>71.90</td>
<td>lr = 0.02, k = 8</td>
<td>lr = 0.2, k = 8</td>
</tr>
</tbody>
</table>
<h3 id="ablation-b-complement-mixing">Ablation B: Complement Mixing</h3>
<p>The idea of “complement mixing” is shown below:</p>
<pre><code>    ```class LabelPropagation(nn.Module):
        &#39;&#39;&#39;
        Label Propagation with:
        y ← α·A^k·y + (1−α)·A_comp^k·y
        where A_comp·y = sum(y)·1 − y − A·y
        &#39;&#39;&#39;
        def __init__(self, k, alpha):
            super().__init__()
            self.k = k
            self.alpha = alpha
            self.A = None

        def preprocess(self, args, edge_index, num_nodes, num_classes):
            self.A = construct_sparse_adj(edge_index, num_nodes=num_nodes, type=&#39;DAD&#39;).to(args.device)

        def apply_A_power(self, A, y, k):
            for _ in range(k):
                y = torch.sparse.mm(A, y)
            return y

        def apply_A_comp_power(self, y, k):
            for _ in range(k):
                sum_y = y.sum(dim=0, keepdim=True)
                Ay = torch.sparse.mm(self.A, y)
                y = sum_y - y - Ay
            return y

        def forward(self, redisuals):
            # One pass of combined propagation
            A_y      = self.apply_A_power(self.A, redisuals, self.k)
            Acomp_y  = self.apply_A_comp_power(redisuals, self.k)
            y_pred   = self.alpha * A_y + (1 - self.alpha) * Acomp_y
            return y_pred```</code></pre>
<p>We swept alpha in the range 0.1 - 0.9 in increments of 0.1, and kept
the same sweep over learning rate and k.</p>
<p>The performance for this ablation was abysmal, to say the least:</p>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 32%" />
<col style="width: 24%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Best Validation Accuracy</th>
<th>Best Test Accuracy</th>
<th>Best Hyperparameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ogbn-arxiv</td>
<td>22.98</td>
<td>21.56</td>
<td>lr = 10.0, k = 1</td>
</tr>
<tr class="even">
<td>cora</td>
<td>45.20</td>
<td>44.80</td>
<td>lr = 0.05, k = 1</td>
</tr>
<tr class="odd">
<td>citeseer</td>
<td>26.00</td>
<td>29.40</td>
<td>lr = 0.1, k = 1</td>
</tr>
<tr class="even">
<td>pubmed</td>
<td>42.60</td>
<td>41.20</td>
<td>lr = 0.1, k = 1</td>
</tr>
</tbody>
</table>
<p>While the idea was to explore if simple injection of long-range and
complementary signals could benefit Residual Propagation, it is clear
that such an approach causes the model to perform poorly. It could
simply be that the complement information is not used well in such a
simple model, injecting little more than noise into the system. Moreover
it is possible that the loss of local structure induced by such an
update could harm more than nonlocal information helps. In conclusion,
while complement mixing is a natural approach to take further, it
appears that RP is far too simple a model to implement it on. A new
model that incorportes nonlocal signals from the start may be a better
candidate altogether to see if simple RP-inspired models can achieve
competitive performance with lower computational overheads.</p>
<h2 id="generalized-residual-propagation">Generalized Residual
Propagation</h2>
<p>Following a structured hyperparameter sweep, we varied the learning
rate (<code>lr</code>) over discrete ranges adapted to each dataset,
based on the <code>run.sh</code> in the original files. For cora and
citeseer, we explored <code>lr</code> values of 5.0, 10.0, and 15.0,
with iteration counts <code>k</code> in {2, 3, 4}. For pubmed, a higher
range of <code>lr</code> in {50, 75, 100, 125} was tested alongside
<code>k</code> in {3, 4, 5}. For heterophilic graphs like roman-empire
and amazon-ratings, lower learning rates (0.5 to 3.0) and shallower
propagation depths (<code>k</code> in {0, 1}) were considered, and 10
runs were used to average performance. The kernel function was also
varied between Gaussian, Cosine, and Sigmoid similarity matrices. All
experiments were logged to <code>run_log.txt</code> (~80 MB), but only
steps with more than 0.01 improvement in accuracy were stored, since
changes across many iterations were marginal. Below, we report the best
performance found for each dataset and kernel.</p>
<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 7%" />
<col style="width: 14%" />
<col style="width: 15%" />
<col style="width: 26%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Kernel</th>
<th>Best Val Acc (%)</th>
<th>Best Test Acc (%)</th>
<th>Best Hyperparameters</th>
<th>Paper Test Accuracy (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>cora</td>
<td>cosine</td>
<td>80.00</td>
<td>81.20</td>
<td>lr = 5.0, k = 4, gamma = 1.0</td>
<td>82.70 *</td>
</tr>
<tr class="even">
<td></td>
<td>gaussian</td>
<td>79.80</td>
<td>82.00</td>
<td>lr = 15.0, k = 3, gamma = 1.0</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>sigmoid</td>
<td>79.20</td>
<td>80.30</td>
<td>lr = 10.0, k = 4, gamma = 1.0</td>
<td></td>
</tr>
<tr class="even">
<td>citeseer</td>
<td>cosine</td>
<td>5.80</td>
<td>7.70</td>
<td>lr = 5.0, k = 2, gamma = 20.0</td>
<td>73.00 *</td>
</tr>
<tr class="odd">
<td></td>
<td>gaussian</td>
<td>75.00</td>
<td>72.70</td>
<td>lr = 10.0, k = 3, gamma = 20.0</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>sigmoid</td>
<td>74.60</td>
<td>72.20</td>
<td>lr = 15.0, k = 4, gamma = 20.0</td>
<td></td>
</tr>
<tr class="odd">
<td>pubmed</td>
<td>cosine</td>
<td>83.40</td>
<td>80.20</td>
<td>lr = 50.0, k = 5, gamma = 1.0</td>
<td>80.10 *</td>
</tr>
<tr class="even">
<td></td>
<td>gaussian</td>
<td>82.60</td>
<td>79.90</td>
<td>lr = 50.0, k = 5, gamma = 1.0</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>sigmoid</td>
<td>82.00</td>
<td>79.60</td>
<td>lr = 125.0, k = 4, gamma = 1.0</td>
<td></td>
</tr>
<tr class="even">
<td>roman-empire</td>
<td>cosine</td>
<td>4.17</td>
<td>4.17</td>
<td>lr = 0.5, k = 0, gamma = 1.0</td>
<td>66.01 ± 0.56 *</td>
</tr>
<tr class="odd">
<td></td>
<td>gaussian</td>
<td>66.65</td>
<td>66.61</td>
<td>lr = 1.0, k = 0, gamma = 1.0</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>sigmoid</td>
<td>40.88</td>
<td>39.66</td>
<td>lr = 1.0, k = 0, gamma = 1.0</td>
<td></td>
</tr>
<tr class="odd">
<td>amazon-ratings</td>
<td>cosine</td>
<td>42.30</td>
<td>41.12</td>
<td>lr = 2.0, k = 1, gamma = 1.0</td>
<td>47.95 ± 0.57</td>
</tr>
<tr class="even">
<td></td>
<td>gaussian</td>
<td>48.78</td>
<td>48.26</td>
<td>lr = 2.0, k = 0, gamma = 1.0</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>sigmoid</td>
<td>40.52</td>
<td>39.47</td>
<td>lr = 1.0, k = 1, gamma = 1.0</td>
<td></td>
</tr>
</tbody>
</table>
<p>Running the whole ablation took about 2 days and 6 hours on a RTX
3060 with 12 GB VRAM.</p>
